"""
M贸dulo de Evaluaci贸n del Modelo
=================================

Este m贸dulo contiene funciones para evaluar el desempe帽o del clasificador
de spam usando diferentes m茅tricas de evaluaci贸n est谩ndar en aprendizaje autom谩tico.

Las m茅tricas implementadas son:
- Accuracy (Exactitud)
- Precision (Precisi贸n)
- Recall (Sensibilidad)
- F1-score
- Matriz de Confusi贸n
"""

from collections import Counter


def matriz_confusion(y_true, y_pred):
    """
    Calcula la matriz de confusi贸n para clasificaci贸n binaria.
    
    La matriz de confusi贸n muestra c贸mo se clasificaron realmente los
    mensajes comparados con las etiquetas verdaderas:
    
    |                    | Predicho Spam | Predicho Ham |
    |--------------------|---------------|--------------|
    | Realmente Spam     | TP (True Positive)  | FN (False Negative) |
    | Realmente Ham      | FP (False Positive) | TN (True Negative)  |
    
    TP (True Positive):  Spam correctamente identificado como spam
    TN (True Negative):  Ham correctamente identificado como ham
    FP (False Positive): Ham incorrectamente identificado como spam
    FN (False Negative): Spam incorrectamente identificado como ham
    
    Par谩metros:
    -----------
    y_true : list
        Lista de etiquetas verdaderas ('spam' o 'ham')
    y_pred : list
        Lista de etiquetas predichas ('spam' o 'ham')
        
    Retorna:
    --------
    dict
        Diccionario con las m茅tricas de la matriz de confusi贸n:
        {
            'TP': True Positives,
            'TN': True Negatives,
            'FP': False Positives,
            'FN': False Negatives
        }
    """
    # Inicializar contadores
    TP = 0  # True Positive: spam predicho correctamente
    TN = 0  # True Negative: ham predicho correctamente
    FP = 0  # False Positive: ham predicho como spam (error tipo I)
    FN = 0  # False Negative: spam predicho como ham (error tipo II)
    
    # Contar cada tipo de predicci贸n
    for verdadero, predicho in zip(y_true, y_pred):
        if verdadero == 'spam' and predicho == 'spam':
            TP += 1
        elif verdadero == 'ham' and predicho == 'ham':
            TN += 1
        elif verdadero == 'ham' and predicho == 'spam':
            FP += 1  # Error: clasificamos ham como spam
        elif verdadero == 'spam' and predicho == 'ham':
            FN += 1  # Error: clasificamos spam como ham
    
    return {
        'TP': TP,
        'TN': TN,
        'FP': FP,
        'FN': FN
    }


def calcular_accuracy(y_true, y_pred):
    """
    Calcula la exactitud (accuracy) del modelo.
    
    La exactitud es la proporci贸n de predicciones correctas sobre el total:
    
    Accuracy = (TP + TN) / (TP + TN + FP + FN)
    
    Es decir, de todos los mensajes, 驴cu谩ntos fueron clasificados correctamente?
    
    Ventajas:
    - F谩cil de entender
    - Buena m茅trica general cuando las clases est谩n balanceadas
    
    Desventajas:
    - Puede ser enga帽osa si hay desbalance de clases
    - No distingue entre tipos de errores
    
    Par谩metros:
    -----------
    y_true : list
        Lista de etiquetas verdaderas ('spam' o 'ham')
    y_pred : list
        Lista de etiquetas predichas ('spam' o 'ham')
        
    Retorna:
    --------
    float
        Exactitud del modelo (entre 0.0 y 1.0, donde 1.0 es perfecto)
        
    Ejemplo:
    --------
    >>> y_true = ['spam', 'ham', 'spam', 'ham']
    >>> y_pred = ['spam', 'ham', 'ham', 'ham']
    >>> calcular_accuracy(y_true, y_pred)
    0.75  # 3 de 4 correctos
    """
    if len(y_true) != len(y_pred):
        raise ValueError("y_true y y_pred deben tener la misma longitud")
    
    if len(y_true) == 0:
        return 0.0
    
    # Contar cu谩ntas predicciones fueron correctas
    correctas = sum(1 for verdadero, predicho in zip(y_true, y_pred) if verdadero == predicho)
    
    # Accuracy = predicciones correctas / total de predicciones
    accuracy = correctas / len(y_true)
    
    return accuracy


def calcular_precision(y_true, y_pred):
    """
    Calcula la precisi贸n (precision) del modelo.
    
    La precisi贸n mide: de todos los mensajes que el modelo predijo como spam,
    驴cu谩ntos realmente eran spam?
    
    Precision = TP / (TP + FP)
    
    Es decir, cuando el modelo dice "esto es spam", 驴qu茅 tan a menudo tiene raz贸n?
    
    Una precisi贸n alta significa que cuando marcamos algo como spam,
    generalmente es correcto (pocos falsos positivos).
    
    Par谩metros:
    -----------
    y_true : list
        Lista de etiquetas verdaderas ('spam' o 'ham')
    y_pred : list
        Lista de etiquetas predichas ('spam' o 'ham')
        
    Retorna:
    --------
    float
        Precisi贸n del modelo (entre 0.0 y 1.0)
        
    Ejemplo:
    --------
    >>> y_true = ['spam', 'spam', 'ham', 'ham']
    >>> y_pred = ['spam', 'spam', 'spam', 'ham']
    >>> calcular_precision(y_true, y_pred)
    0.67  # 2 TP / (2 TP + 1 FP) = 2/3
    """
    matriz = matriz_confusion(y_true, y_pred)
    TP = matriz['TP']
    FP = matriz['FP']
    
    # Si no hay predicciones positivas (spam), no podemos calcular precisi贸n
    if TP + FP == 0:
        return 0.0
    
    # Precision = True Positives / (True Positives + False Positives)
    precision = TP / (TP + FP)
    
    return precision


def calcular_recall(y_true, y_pred):
    """
    Calcula la sensibilidad (recall) del modelo.
    
    El recall mide: de todos los mensajes que realmente son spam,
    驴cu谩ntos logr贸 identificar el modelo?
    
    Recall = TP / (TP + FN)
    
    Tambi茅n se llama "Sensibilidad" o "Tasa de Verdaderos Positivos".
    
    Un recall alto significa que capturamos la mayor铆a del spam
    (pocos falsos negativos - no dejamos pasar mucho spam).
    
    Par谩metros:
    -----------
    y_true : list
        Lista de etiquetas verdaderas ('spam' o 'ham')
    y_pred : list
        Lista de etiquetas predichas ('spam' o 'ham')
        
    Retorna:
    --------
    float
        Recall del modelo (entre 0.0 y 1.0)
        
    Ejemplo:
    --------
    >>> y_true = ['spam', 'spam', 'spam', 'ham']
    >>> y_pred = ['spam', 'ham', 'spam', 'ham']
    >>> calcular_recall(y_true, y_pred)
    0.67  # 2 TP / (2 TP + 1 FN) = 2/3
    """
    matriz = matriz_confusion(y_true, y_pred)
    TP = matriz['TP']
    FN = matriz['FN']
    
    # Si no hay casos positivos reales (spam), no podemos calcular recall
    if TP + FN == 0:
        return 0.0
    
    # Recall = True Positives / (True Positives + False Negatives)
    recall = TP / (TP + FN)
    
    return recall


def calcular_f1_score(y_true, y_pred):
    """
    Calcula el F1-score del modelo.
    
    El F1-score es la media arm贸nica entre precisi贸n y recall:
    
    F1 = 2 * (Precision * Recall) / (Precision + Recall)
    
    Es una m茅trica balanceada que combina precisi贸n y recall en un solo n煤mero.
    
    驴Por qu茅 media arm贸nica y no aritm茅tica?
    - La media arm贸nica penaliza m谩s cuando una de las dos m茅tricas es muy baja
    - Si precision o recall es muy bajo, el F1-score tambi茅n ser谩 bajo
    - Esto nos ayuda a encontrar un balance entre ambos
    
    El F1-score es 煤til cuando queremos un balance entre:
    - No marcar demasiados correos leg铆timos como spam (alta precisi贸n)
    - No dejar pasar demasiado spam (alto recall)
    
    Par谩metros:
    -----------
    y_true : list
        Lista de etiquetas verdaderas ('spam' o 'ham')
    y_pred : list
        Lista de etiquetas predichas ('spam' o 'ham')
        
    Retorna:
    --------
    float
        F1-score del modelo (entre 0.0 y 1.0)
        
    Ejemplo:
    --------
    >>> precision = 0.8
    >>> recall = 0.75
    >>> f1 = 2 * (0.8 * 0.75) / (0.8 + 0.75)  # = 0.774
    """
    precision = calcular_precision(y_true, y_pred)
    recall = calcular_recall(y_true, y_pred)
    
    # Si ambas m茅tricas son 0, el F1-score es 0
    if precision + recall == 0:
        return 0.0
    
    # F1-score = 2 * (Precision * Recall) / (Precision + Recall)
    f1_score = 2 * (precision * recall) / (precision + recall)
    
    return f1_score


def evaluar_modelo(modelo, X_test, y_test):
    """
    Eval煤a un modelo completo calculando todas las m茅tricas disponibles.
    
    Esta funci贸n realiza predicciones sobre el conjunto de prueba y calcula
    todas las m茅tricas de evaluaci贸n: accuracy, precision, recall, F1-score
    y la matriz de confusi贸n.
    
    Par谩metros:
    -----------
    modelo : NaiveBayesSpamDetector
        El modelo entrenado que se desea evaluar
    X_test : list
        Lista de mensajes de prueba (cada uno es una lista de tokens)
    y_test : list
        Lista de etiquetas verdaderas correspondientes
        
    Retorna:
    --------
    dict
        Diccionario con todas las m茅tricas:
        {
            'accuracy': float,
            'precision': float,
            'recall': float,
            'f1_score': float,
            'matriz_confusion': {
                'TP': int,
                'TN': int,
                'FP': int,
                'FN': int
            }
        }
    """
    # Realizar predicciones sobre todos los mensajes de prueba
    y_pred = []
    for mensaje in X_test:
        prediccion = modelo.predecir(mensaje)
        y_pred.append(prediccion)
    
    # Calcular todas las m茅tricas
    accuracy = calcular_accuracy(y_test, y_pred)
    precision = calcular_precision(y_test, y_pred)
    recall = calcular_recall(y_test, y_pred)
    f1_score = calcular_f1_score(y_test, y_pred)
    matriz = matriz_confusion(y_test, y_pred)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'matriz_confusion': matriz
    }


def imprimir_resultados(resultados):
    """
    Imprime los resultados de la evaluaci贸n de forma legible.
    
    Par谩metros:
    -----------
    resultados : dict
        Diccionario con los resultados de evaluar_modelo()
    """
    print("\n" + "="*60)
    print("RESULTADOS DE LA EVALUACIN")
    print("="*60)
    
    print(f"\n M茅tricas de Desempe帽o:\n")
    print(f"  Accuracy  (Exactitud):    {resultados['accuracy']:.4f} ({resultados['accuracy']*100:.2f}%)")
    print(f"  Precision (Precisi贸n):    {resultados['precision']:.4f} ({resultados['precision']*100:.2f}%)")
    print(f"  Recall    (Sensibilidad): {resultados['recall']:.4f} ({resultados['recall']*100:.2f}%)")
    print(f"  F1-Score:                 {resultados['f1_score']:.4f} ({resultados['f1_score']*100:.2f}%)")
    
    matriz = resultados['matriz_confusion']
    print(f"\n Matriz de Confusi贸n:\n")
    print(f"                    Predicho")
    print(f"                  Spam    Ham")
    print(f"  Realmente Spam   {matriz['TP']:4d}   {matriz['FN']:4d}")
    print(f"  Realmente Ham    {matriz['FP']:4d}   {matriz['TN']:4d}")
    
    print("\n" + "="*60 + "\n")

